{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3811e6e2-fae4-48fe-92ce-d19930dda37a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](img/330-banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f25497-8be5-480a-854c-cf728b6da55c",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Lecture 14: Midterm review\n",
    "\n",
    "UBC 2022-23\n",
    "\n",
    "Instructor: Varada Kolhatkar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc0bcc5-d8ae-45d6-ab33-81b3d9d15c92",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c58c8084-8c94-4ac8-9550-8e5fb8991bbb",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import pandas as pd\n",
    "from sklearn.compose import (\n",
    "    ColumnTransformer,\n",
    "    TransformedTargetRegressor,\n",
    "    make_column_transformer,\n",
    ")\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, RidgeCV\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920f1dad-5511-4afc-a251-35ca8b00c314",
   "metadata": {},
   "source": [
    "## Terminologies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b852f6-a4cc-43fc-ad97-52bd6944fc95",
   "metadata": {},
   "source": [
    "- **Supervised learning** ([Gmail spam filtering])\n",
    "    - Training a model from input data and its corresponding targets to predict targets for new examples.\n",
    "    - **Classification vs. Regression**\n",
    "        - **Classification problem**: predicting among two or more discrete classes\n",
    "            - Example1: Predict whether a patient has a liver disease or not\n",
    "            - Example2: Predict whether a student would get an A+ or not in quiz2.  \n",
    "        - **Regression problem**: predicting a continuous value\n",
    "            - Example1: Predict housing prices \n",
    "            - Example2: Predict a student's score in quiz2.\n",
    "- **Unsupervised learning** ([Google News](https://news.google.com/))\n",
    "    - Training a model to find patterns in a dataset, typically an unlabeled dataset.\n",
    "    - training data consists of observations ($X$) **without any corresponding targets**\n",
    "- Supervised machine learning is about function approximation, i.e., finding the mapping function between `X` and `y` whereas unsupervised machine learning is about concisely describing the data.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a454b2d-19b7-4d65-9319-7252d9af0c49",
   "metadata": {},
   "source": [
    "- **Features** \n",
    ": columns of input data. Features are relevant characteristics of the problem, usually suggested by experts. Features are typically denoted by $X$ and the number of features is usually denoted by $d$.  \n",
    "\n",
    "- **Target**\n",
    ": Target is the feature we want to predict (typically denoted by $y$). \n",
    "\n",
    "- **Example** \n",
    ": A row of feature values. When people refer to an example, it may or may not include the target corresponding to the feature values, depending upon the context. The number of examples is usually denoted by $n$. \n",
    "\n",
    "- **Training**\n",
    ": The process of learning the mapping between the features ($X$) and the target ($y$). Fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888529a6-1cad-4e47-85ea-77ed74ad46ea",
   "metadata": {},
   "source": [
    "### `DummyClassifier` \n",
    "\n",
    "- `sklearn`'s baseline model for classification \n",
    "- **Baseline**\n",
    ": A simple machine learning algorithm based on simple rules of thumb. \n",
    "\n",
    "    - For example, most frequent baseline always predicts the most frequent label in the training set. \n",
    "    - Baselines provide a way to sanity check your machine learning model.  \n",
    "    - Baselines serve as reference points in ML workflow. \n",
    "### Steps to train a classifier using `sklearn` \n",
    "\n",
    "1. Read the data\n",
    "2. Create $X$ and $y$\n",
    "3. Create a classifier object\n",
    "4. `fit` the classifier\n",
    "5. `predict` on new examples\n",
    "6. `score` the model\n",
    "    - `score` gives the **accuracy** of the model, i.e., proportion of correctly predicted targets. \n",
    "    - **error**, which is usually $1 - accuracy$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504bddbd-18ee-4ea0-8163-03005a723648",
   "metadata": {},
   "source": [
    "### [`DummyRegressor`](https://scikit-learn.org/0.15/modules/generated/sklearn.dummy.DummyRegressor.html)\n",
    "regression problems use `DummyRegressor`, which predicts mean, median, or constant value of the training set for all examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbc9b42-fd3d-45aa-bedf-5b785b821035",
   "metadata": {},
   "source": [
    "### `DecisionTree` \n",
    "- are models that make predictions by sequentially looking at features and checking whether they are above/below a threshold\n",
    "- learn a hierarchy of if/else questions, similar to questions you might ask in a 20-questions game.       \n",
    "- learn axis-aligned decision boundaries (vertical and horizontal lines with 2 features)    \n",
    "- One way to control the complexity of decision tree models is by using the depth hyperparameter (`max_depth` in `sklearn`). \n",
    "- **Predict**\n",
    "    - Start at the top of the tree. Ask binary questions at each node and follow the appropriate path in the tree. Once you are at a leaf node, you have the prediction. \n",
    "    - Note that the model only considers the features which are in the learned tree and ignores all other features. \n",
    "- **fit**\n",
    "    - Each node either represents a question or an answer. The terminal nodes (called leaf nodes) represent answers. \n",
    "    - Which features are most useful for classification? \n",
    "    - Minimize **impurity** at each question\n",
    "    - Common criteria to minimize impurity: [gini index](https://scikit-learn.org/stable/modules/tree.html#classification-criteria), information gain, cross entropy\n",
    "### Decision tree for regression problems\n",
    "\n",
    "- We can also use decision tree algorithm for regression. \n",
    "- Instead of gini, we use [some other criteria](https://scikit-learn.org/stable/modules/tree.html#mathematical-formulation) for splitting. A common one is mean squared error (MSE). (More on this in later videos.)\n",
    "- `scikit-learn` supports regression using decision trees with `DecisionTreeRegressor` \n",
    "    - `fit` and `predict` paradigms similar to classification\n",
    "    - `score` returns somethings called [$R^2$ score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score).     \n",
    "        - The maximum $R^2$ is 1 for perfect predictions. \n",
    "        - It can be negative which is very bad (worse than `DummyRegressor`). \n",
    "### Parameters \n",
    "\n",
    "- The decision tree algorithm primarily learns two things: \n",
    "    - the best feature to split on\n",
    "    - the threshold for the feature to split on at each node\n",
    "- These are called **parameters** of the decision tree model.  \n",
    "\n",
    "**Parameters**\n",
    ": When you call `fit`, a bunch of values get set, like the features to split on and split thresholds. These are called **parameters**. These are learned by the algorithm from the data during training. We need them during prediction time. \n",
    "\n",
    "**Hyperparameters**\n",
    ": Even before calling `fit` on a specific data set, we can set some \"knobs\" that control the learning. These are called **hyperparameters**. These are specified based on: expert knowledge, heuristics, or systematic/automated optimization (more on this in the coming lectures).  \n",
    "\n",
    "### Max-depth is a **hyperparameter** of `DecisionTreeClassifier`. \n",
    "- With the default setting, the nodes are expanded until all leaves are \"pure\". \n",
    "- The decision tree is creating very specific rules, based on just one example from the data. \n",
    "- Is it possible to control the learning in any way? \n",
    "    - Yes! One way to do it is by controlling the **depth** of the tree, which is the length of the longest path from the tree root to a leaf. \n",
    "- **Decision stump**\n",
    ": A decision tree with only one split (depth=1) is called a **decision stump**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13b4af6-155d-405b-895d-f92b9d1faa4a",
   "metadata": {},
   "source": [
    "### `k-nearest neighbours`\n",
    "- Given a new data point, predict the class of the data point by finding the \"closest\" data point in the training set, i.e., by finding its \"nearest neighbour\" or majority vote of nearest neighbours. \n",
    "- knn = KNeighborsClassifier(n_neighbors=k)\n",
    "- `weights` $\\rightarrow$ When predicting label, you can assign higher weight to the examples which are closer to the query example.  \n",
    "- For regular $k$-NN for supervised learning (not with sparse matrices), you should scale your features.\n",
    "### Pros of $k$-NNs for supervised learning\n",
    "\n",
    "- Easy to understand, interpret.\n",
    "- Simple hyperparameter $k$ (`n_neighbors`) controlling the fundamental tradeoff.\n",
    "- Can learn very complex functions given enough data.\n",
    "- Lazy learning: Takes no time to `fit`\n",
    "### Cons of $k$-NNs for supervised learning\n",
    "- Can be potentially be VERY slow during prediction time, especially when the training set is very large. \n",
    "- Often not that great test accuracy compared to the modern approaches.\n",
    "- It does not work well on datasets with many features or where most feature values are 0 most of the time (sparse datasets).   \n",
    "### Curse of dimensionality\n",
    "\n",
    "- Affects all learners but especially bad for nearest-neighbour. \n",
    "- $k$-NN usually works well when the number of dimensions $d$ is small but things fall apart quickly as $d$ goes up.\n",
    "- If there are many irrelevant attributes, $k$-NN is hopelessly confused because all of them contribute to finding similarity between examples. \n",
    "- With enough irrelevant attributes the accidental similarity swamps out meaningful similarity and $k$-NN is no better than random guessing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b1a9c1-d167-4cb9-b943-16a61572d28f",
   "metadata": {},
   "source": [
    "### `Support Vector Machines (SVMs) with RBF kernel`\n",
    "- Another popular similarity-based algorithm is Support Vector Machines with RBF Kernel (SVM RBFs)\n",
    "- Superficially, SVM RBFs are more like weighted $k$-NNs.\n",
    "    - The decision boundary is defined by **a set of positive and negative examples** and **their weights** together with **their similarity measure**. \n",
    "    - A test example is labeled positive if on average it looks more like positive examples than the negative examples. \n",
    "<br>\n",
    "- The primary difference between $k$-NNs and SVM RBFs is that \n",
    "    - Unlike $k$-NNs, SVM RBFs only remember the key examples (support vectors). \n",
    "    - SVMs use a different similarity metric which is called a \"kernel\". A popular kernel is Radial Basis Functions (RBFs)\n",
    "    - They usually perform better than $k$-NNs! \n",
    "- We can think of SVM with RBF kernel as \"smooth KNN\". \n",
    "### Support vectors \n",
    "\n",
    "- Each training example either is or isn't a \"support vector\".\n",
    "  - This gets decided during `fit`.\n",
    "\n",
    "- **Main insight: the decision boundary only depends on the support vectors.**\n",
    "### Hyperparameters of SVM \n",
    "\n",
    "- Key hyperparameters of `rbf` SVM are\n",
    "    - `gamma`\n",
    "        - #### Relation of `gamma` and the fundamental trade-off\n",
    "            - `gamma` controls the complexity (fundamental trade-off), just like other hyperparameters we've seen.\n",
    "              - larger `gamma` $\\rightarrow$ more complex\n",
    "              - smaller `gamma` $\\rightarrow$ less complex\n",
    "    - `C`\n",
    "        - #### Relation of `C` and the fundamental trade-off\n",
    "            - `C` _also_ affects the fundamental tradeoff\n",
    "                - larger `C` $\\rightarrow$ more complex \n",
    "                - smaller `C` $\\rightarrow$ less complex "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9ddb8d-92fa-4d72-a0ce-327a7b43fb0f",
   "metadata": {},
   "source": [
    "### How to approximate generalization error? \n",
    "\n",
    "A common way is **data splitting**. \n",
    "- split on X and y\n",
    "    - X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "- split on df\n",
    "    - train_df, test_df = train_test_split(df, test_size=0.2, random_state=123)\n",
    "- validation data\n",
    "    - for hyperparameter tuning\n",
    "    - don't pass into fit\n",
    "\n",
    "### Summary of train, validation, test, and deployment data\n",
    "\n",
    "|         | `fit` | `score` | `predict` |\n",
    "|----------|-------|---------|-----------|\n",
    "| Train    | ✔️      | ✔️      | ✔️         |\n",
    "| Validation |      | ✔️      | ✔️         |\n",
    "| Test    |       |  once   | once         |\n",
    "| Deployment    |       |       | ✔️         |\n",
    "\n",
    "You can typically expect $E_{train} < E_{validation} < E_{test} < E_{deployment}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c3718e-295e-408d-a05d-e23587ef2620",
   "metadata": {},
   "source": [
    "### Cross-validation\n",
    "- problem: when dataset is too small, train/validation data will also be small, you might end up with results that doesn't represent your test data well. This is when you want to use cross-validation\n",
    "- Split the data into $k$ folds ($k>2$, often $k=10$). In the picture below $k=4$.\n",
    "- Each \"fold\" gets a turn at being the validation set, each fold gets a score\n",
    "- The purpose of cross-validation is to **evaluate** how well the model will generalize to unseen data. \n",
    "- Gives a more **robust** measure of error on unseen data.\n",
    "#### `cross_val_score`\n",
    "- Gives us a list of validation scores in each fold.\n",
    "#### `cross_validate`\n",
    "- Gives us access to training and validation scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8502a8-04ea-49f6-9f30-8f3f4d9659b1",
   "metadata": {},
   "source": [
    "#### Our typical supervised learning set up is as follows: \n",
    "\n",
    "- We are given training data with features `X` and target `y`\n",
    "- We split the data into train and test portions: `X_train, y_train, X_test, y_test`\n",
    "- We carry out hyperparameter optimization using cross-validation on the train portion: `X_train` and `y_train`. \n",
    "- We assess our best performing model on the test portion: `X_test` and `y_test`.  \n",
    "- What we care about is the **test error**, which tells us how well our model can be generalized.\n",
    "- If this test error is \"reasonable\" we deploy the model which will be used on new unseen examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2b436b-5c58-4c50-b77c-1f947b4b54b5",
   "metadata": {},
   "source": [
    "### Types of errors\n",
    "- $E_\\textrm{train}$ is your training error (or mean train error from cross-validation).\n",
    "- $E_\\textrm{valid}$ is your validation error (or mean validation error from cross-validation).\n",
    "- $E_\\textrm{test}$ is your test error.\n",
    "- $E_\\textrm{best}$ is the best possible error you could get for a given problem.\n",
    "### Underfitting \n",
    "- If your model is too simple, like `DummyClassifier` or `DecisionTreeClassifier` with `max_depth=1`, it's not going to pick up on some random quirks in the data but it won't even capture useful patterns in the training data.\n",
    "- The model won't be very good in general. Both train and validation errors would be high (or train and validation scores are low). This is **underfitting**.\n",
    "- The gap between train and validation error is going to be lower.\n",
    "- $E_\\textrm{best} \\lt E_\\textrm{train} \\lesssim E_\\textrm{valid}$\n",
    "### Overfitting \n",
    "- If your model is very complex, like a `DecisionTreeClassifier(max_depth=None)`, then you will learn unreliable patterns in order to get every single training example correct.\n",
    "- The training error is going to be very low (training score is high), but there will be a big gap between the training error and the validation error (big gap between training score and validation score). This is **overfitting**.\n",
    "- In overfitting scenario, usually we'll see: \n",
    "$E_\\textrm{train} \\lt E_\\textrm{best}  \\lt E_\\textrm{valid}$\n",
    "- In general, if $E_\\textrm{train}$ is low, we are likely to be in the overfitting scenario. It is fairly common to have at least a bit of this.\n",
    "### The \"fundamental tradeoff\" of supervised learning:\n",
    "- **As you increase model complexity, $E_\\textrm{train}$ tends to go down (training score goes up) but $E_\\textrm{valid}-E_\\textrm{train}$ tends to go up (the gap between training score and validation score goes up).**\n",
    "### How to pick a model that would generalize best?\n",
    "- There are many subtleties here and there is no perfect answer but a  common practice is to pick the model with minimum cross-validation error (best validation score). \n",
    "### The golden rule <a name=\"4\"></a>\n",
    "- Even though we care the most about test error **THE TEST DATA CANNOT INFLUENCE THE TRAINING PHASE IN ANY WAY**. \n",
    "- should never call **fit** on validation and test data\n",
    "- To avoid violating the golden rule:\n",
    "    - **Splitting**: Before doing anything, split the data `X` and `y` into `X_train`, `X_test`, `y_train`, `y_test` or `train_df` and `test_df` using `train_test_split`. \n",
    "    - **Select the best model using cross-validation**: Use `cross_validate` with `return_train_score = True` so that we can get access to training scores in each fold. (If we want to plot train vs validation error plots, for instance.) \n",
    "    - **Scoring on test data**: Finally score on the test data with the chosen hyperparameters to examine the generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4568c7cf-39e5-48c7-b2e7-a6b0ff49878f",
   "metadata": {},
   "source": [
    "## ML fundamentals\n",
    "\n",
    "- What are four types of data we have seen so far? \n",
    "    - train data, test data, validation data, deployment data\n",
    "- What are the advantages of cross-validation?\n",
    "    - cross-validation allows us to split data into k-folds, each fold takes turn to be the validation set, if we have a small dataset, the train and validation data might not be a good representation of our test data, doing this will get a more robust estimate on model performance. \n",
    "    - makes better use of data\n",
    "    - can know if model is too sensitive to the given training data\n",
    "- Why do we split data?\n",
    "    - so that we can get a robust estimate of the model, splitting data helps us better generalized unseen examples\n",
    "- Why it's important to look at sub-scores of cross-validation?\n",
    "- What is the fundamental trade-off in supervised machine learning?\n",
    "- What is the Golden rule? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee64fa1f-e3fc-4b3d-a47c-d11cff75c1d4",
   "metadata": {},
   "source": [
    "### Dimensions in ML problems \n",
    "\n",
    "In ML, usually we deal with high dimensional problems where examples are hard to visualize.  \n",
    "\n",
    "- $d \\approx 20$ is considered low dimensional\n",
    "- $d \\approx 1000$ is considered medium dimensional \n",
    "- $d \\approx 100,000$ is considered high dimensional \n",
    "\n",
    "### Feature vectors \n",
    "**Feature vector**\n",
    ": is composed of feature values associated with an example.\n",
    "### Distance between feature vectors \n",
    "\n",
    "- A common way to calculate the distance between vectors is calculating the **Euclidean distance**. \n",
    "- The euclidean distance between vectors $u = <u_1, u_2, \\dots, u_n>$ and $v = <v_1, v_2, \\dots, v_n>$ is defined as: \n",
    "\n",
    "$$distance(u, v) = \\sqrt{\\sum_{i =1}^{n} (u_i - v_i)^2}$$ \n",
    "- Subtract the two cities, Square the difference, Sum them up, Take the square root\n",
    "- In sklearn: euclidean_distances(two_things)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6efe36-72ac-4244-97f3-9e47f99e7f84",
   "metadata": {},
   "source": [
    "## Pros and cons of different ML models\n",
    "\n",
    "- Decision trees\n",
    "- KNNs, SVM RBFs\n",
    "- Linear models \n",
    "- Random forests\n",
    "- LGBM, CatBoost\n",
    "- Stacking Averaging "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888bd743-456a-4f6f-b5ef-139c52d6818e",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d86856c-392c-484d-9b0a-ea2d4fce60bc",
   "metadata": {},
   "source": [
    "### `StandardScaler`\n",
    "- We'll use `scikit-learn`'s [`StandardScaler`], which is a `transformer`.   \n",
    "| Approach | What it does | How to update $X$ (but see below!) | sklearn implementation | \n",
    "|---------|------------|-----------------------|----------------|\n",
    "| standardization | sets sample mean to $0$, s.d. to $1$   | `X -= np.mean(X,axis=0)`<br>`X /=  np.std(X,axis=0)` | [`StandardScaler()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) |\n",
    "### `fit` and `transform` paradigm for transformers\n",
    "- `sklearn` uses `fit` and `transform` paradigms for feature transformations. \n",
    "- We `fit` the transformer on the train split and then `transform` the train split as well as the test split. \n",
    "### `sklearn` API summary: transformers\n",
    "\n",
    "```\n",
    "transformer.fit(X_train, [y_train])\n",
    "X_train_transformed = transformer.transform(X_train)\n",
    "X_test_transformed = transformer.transform(X_test)\n",
    "```  \n",
    "- You can pass `y_train` in `fit` but it's usually ignored. It allows you to pass it just to be consistent with usual usage of `sklearn`'s `fit` method.   \n",
    "- You can also carry out fitting and transforming in one call using `fit_transform`. But be mindful to use it only on the train split and **not** on the test split. \n",
    "### Common preprocessing techniques\n",
    "- Imputation: Tackling missing values\n",
    "- Scaling: Scaling of numeric features\n",
    "- One-hot encoding: Tackling categorical variables \n",
    "### `SimpleImputer`\n",
    "- `SimpleImputer` is a transformer in `sklearn` to deal with this problem. For example, \n",
    "    - You can impute missing values in categorical columns with the most frequent value.\n",
    "    - You can impute the missing values in numeric columns with the mean or median of the column.    \n",
    "### Breaking the golden rule\n",
    "- Are we applying `fit_transform` on train portion and `transform` on validation portion in each fold?  \n",
    "    - Here you might be allowing information from the validation set to **leak** into the training step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecd931d-ec19-452c-a8b5-294315f5d6e2",
   "metadata": {},
   "source": [
    "### Pipelines\n",
    "\n",
    "- [`scikit-learn Pipeline`] allows you to define a \"pipeline\" of transformers with a final estimator.\n",
    "- Using a `Pipeline` takes care of applying the `fit_transform` on the train portion and only `transform` on the validation portion in each fold.   \n",
    "- #### Pipeline() \n",
    "    - Syntax: pass in a list of steps.\n",
    "    - The last step should be a **model/classifier/regressor**.\n",
    "    - All the earlier steps should be **transformers**.\n",
    "    ```\n",
    "    pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"regressor\", KNeighborsRegressor()),\n",
    "        ]\n",
    "    )\n",
    "    ```\n",
    "- #### Make_pipeline() \n",
    "    - Does not permit naming steps\n",
    "    - Instead the names of steps are set to lowercase of their types automatically; `StandardScaler()` would be named as `standardscaler`\n",
    "    ```\n",
    "    pipe = make_pipeline(\n",
    "        SimpleImputer(strategy=\"median\"), StandardScaler(), KNeighborsRegressor()\n",
    ")\n",
    "    ```\n",
    "- #### .named_steps[\"NAME OF THE STEP\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf30b454-9f43-481e-9b1c-da43031fc0d8",
   "metadata": {},
   "source": [
    "Let's bring back our quiz2 grades toy dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64a854ff-7887-40d0-b2fe-a1c3c0189b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enjoy_course</th>\n",
       "      <th>ml_experience</th>\n",
       "      <th>major</th>\n",
       "      <th>class_attendance</th>\n",
       "      <th>university_years</th>\n",
       "      <th>lab1</th>\n",
       "      <th>lab2</th>\n",
       "      <th>lab3</th>\n",
       "      <th>lab4</th>\n",
       "      <th>quiz1</th>\n",
       "      <th>quiz2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>3</td>\n",
       "      <td>92</td>\n",
       "      <td>93.0</td>\n",
       "      <td>84</td>\n",
       "      <td>91</td>\n",
       "      <td>92</td>\n",
       "      <td>A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Mechanical Engineering</td>\n",
       "      <td>Average</td>\n",
       "      <td>2</td>\n",
       "      <td>94</td>\n",
       "      <td>90.0</td>\n",
       "      <td>80</td>\n",
       "      <td>83</td>\n",
       "      <td>91</td>\n",
       "      <td>not A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Mathematics</td>\n",
       "      <td>Poor</td>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>85.0</td>\n",
       "      <td>83</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>not A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>Mathematics</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>3</td>\n",
       "      <td>91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92</td>\n",
       "      <td>91</td>\n",
       "      <td>89</td>\n",
       "      <td>A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Psychology</td>\n",
       "      <td>Good</td>\n",
       "      <td>4</td>\n",
       "      <td>77</td>\n",
       "      <td>83.0</td>\n",
       "      <td>90</td>\n",
       "      <td>92</td>\n",
       "      <td>85</td>\n",
       "      <td>A+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  enjoy_course  ml_experience                   major class_attendance  \\\n",
       "0          yes              1        Computer Science        Excellent   \n",
       "1          yes              1  Mechanical Engineering          Average   \n",
       "2          yes              0             Mathematics             Poor   \n",
       "3           no              0             Mathematics        Excellent   \n",
       "4          yes              0              Psychology             Good   \n",
       "\n",
       "   university_years  lab1  lab2  lab3  lab4  quiz1   quiz2  \n",
       "0                 3    92  93.0    84    91     92      A+  \n",
       "1                 2    94  90.0    80    83     91  not A+  \n",
       "2                 3    78  85.0    83    80     80  not A+  \n",
       "3                 3    91   NaN    92    91     89      A+  \n",
       "4                 4    77  83.0    90    92     85      A+  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grades_df = pd.read_csv('data/quiz2-grade-toy-col-transformer.csv')\n",
    "grades_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e081808-d250-40a2-8500-0fba312533a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = grades_df.drop(columns=['quiz2']), grades_df['quiz2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f74c704-4c1a-42ef-8838-3f0ae6b36a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feats = [\"university_years\", \"lab1\", \"lab3\", \"lab4\", \"quiz1\"]  # apply scaling\n",
    "categorical_feats = [\"major\"]  # apply one-hot encoding\n",
    "passthrough_feats = [\"ml_experience\"]  # do not apply any transformation\n",
    "drop_feats = [\n",
    "    \"lab2\",\n",
    "    \"class_attendance\",\n",
    "    \"enjoy_course\",\n",
    "]  # do not include these features in modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef7234a-0368-4716-876e-ef0da500c0a2",
   "metadata": {},
   "source": [
    "- What's the difference between sklearn estimators and transformers?  \n",
    "- Can you think of a better way to impute missing values compared to `SimpleImputer`? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b10938-8e20-4be4-aefd-22b686c4851f",
   "metadata": {},
   "source": [
    "### One-hot encoding \n",
    "- Here we simply assign an integer to each of our unique categorical labels. \n",
    "- What's the purpose of the following arguments of one-hot encoding?\n",
    "    - handle_unknown=\"ignore\"\n",
    "        - some categories show up very rarely, not present in training data\n",
    "        - if not used, when you want to transform data, it does not know how to handle\n",
    "        - everything will be encoded as 0\n",
    "        - if you have many unknown values, all of them will be 0\n",
    "    - sparse=False\n",
    "        - have many rows and categorical columns with many categories, you will not want to use this\n",
    "        - don't really want to use this\n",
    "        - if you know that there are many 0's, and only few values that are not 0, you want to keep track of those that are not 0's\n",
    "    - drop=\"if_binary\"    \n",
    "        - when you know that there are only two categories for this column, use 0 and 1 to encode the information there\n",
    "- How do you deal with categorical features with only two possible categories? \n",
    "### `OneHotEncoder` and sparse features \n",
    "- By default, `OneHotEncoder` also creates sparse features. \n",
    "- You could set `sparse=False` to get a regular `numpy` array. \n",
    "- If there are a huge number of categories, it may be beneficial to keep them sparse.\n",
    "- For smaller number of categories, it doesn't matter much."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cf3d09-b521-476b-80cc-ea9e87ad191e",
   "metadata": {},
   "source": [
    "### Ordinal encoding\n",
    "- Create new binary columns to represent our categories.\n",
    "- If we have $c$ categories in our column.\n",
    "    - We create $c$ new binary columns to represent those categories.\n",
    "- What's the difference between ordinal encoding and one-hot encoding? \n",
    "    - one-hot encoding will create new columns for new categories\n",
    "- What happens if we do not order the categories when we apply ordinal encoding?  Does it matter if we order the categories in ascending or descending order? \n",
    "    - if we dont do order, it will alphabetically order the numbers, and we dont want that, we want our model to capture that excellent and good are closer to each other instead of excellent and bad.\n",
    "    - it doesnt really matter\n",
    "- What would happen if an unknown category shows up during validation or test time during ordinal encoding? For example, for `class_attendance` feature what if a category called \"super poor\" shows up? \n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca74bc0b-3175-4c9d-be61-cef9b8bee30a",
   "metadata": {},
   "source": [
    "#### OHE vs. ordinal encoding\n",
    "\n",
    "- Since `enjoy_course` feature is binary you decide to apply one-hot encoding with `drop=\"if_binary\"`. Your friend decide to apply ordinal encoding on it. Will it make any difference in the transformed data? \n",
    "    - there wont be any difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3037d867-ed9d-4a08-b027-3dea90c8d2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(drop=\"if_binary\", sparse=False)\n",
    "ohe_encoded = ohe.fit_transform(grades_df[['enjoy_course']]).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07f82887-36f1-428d-a423-c70ffd104c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "oe = OrdinalEncoder()\n",
    "oe_encoded = oe.fit_transform(grades_df[['enjoy_course']]).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f89b3cb-b42d-40b5-94e7-c6dfbf010c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oe_encoded</th>\n",
       "      <th>ohe_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    oe_encoded  ohe_encoded\n",
       "0          1.0          1.0\n",
       "1          1.0          1.0\n",
       "2          1.0          1.0\n",
       "3          0.0          0.0\n",
       "4          1.0          1.0\n",
       "5          0.0          0.0\n",
       "6          1.0          1.0\n",
       "7          0.0          0.0\n",
       "8          0.0          0.0\n",
       "9          1.0          1.0\n",
       "10         1.0          1.0\n",
       "11         1.0          1.0\n",
       "12         1.0          1.0\n",
       "13         1.0          1.0\n",
       "14         0.0          0.0\n",
       "15         0.0          0.0\n",
       "16         1.0          1.0\n",
       "17         1.0          1.0\n",
       "18         0.0          0.0\n",
       "19         0.0          0.0\n",
       "20         1.0          1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = { \"oe_encoded\": oe_encoded, \n",
    "         \"ohe_encoded\": ohe_encoded}\n",
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b71e80-fe7e-446d-b54c-a88d06fc48c1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- In what scenarios it's OK to break the golden rule?\n",
    "    - if you are using one hot encoding or ordinal encoding, and you know in advance what possible categories there are for these features, then its ok\n",
    "    - sometimes its ok to incorporate human knowledge in the model\n",
    "- What are possible ways to deal with categorical columns with large number of categories? \n",
    "    - group together different categories\n",
    "    - treat it as binary features\n",
    "    - take the most frequent occurring values\n",
    "- In what scenarios you'll not include a feature in your model even if it's a good predictor? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2189935c-5e5a-4b74-bfa2-c97af52ce5df",
   "metadata": {},
   "source": [
    "### `ColumnTransformer`\n",
    "- we want to apply different transformation on different columns. \n",
    "- define numerical_features, passthrough_features, categorical_features, and drop_features\n",
    "- ColumnTransformer()\n",
    "- make_column_transformer()\n",
    "```\n",
    "ct = make_column_transformer(    \n",
    "    (StandardScaler(), numeric_feats),  # scaling on numeric features\n",
    "    (\"passthrough\", passthrough_feats),  # no transformations on the binary features    \n",
    "    (OneHotEncoder(), categorical_feats),  # OHE on categorical features\n",
    "    (\"drop\", drop_feats),  # drop the drop features\n",
    ")\n",
    "```\n",
    "- viewing dataframe after transformation\n",
    "    ```\n",
    "    column_names = (\n",
    "        numeric_feats\n",
    "        + passthrough_feats    \n",
    "        + ct.named_transformers_[\"onehotencoder\"].get_feature_names_out().tolist()\n",
    "    )\n",
    "    pd.DataFrame(transformed, columns=column_names)\n",
    "    ```\n",
    "#### Cases where it's OK to break the golden rule \n",
    "\n",
    "- If it's some fix number of categories. For example, if it's something like provinces in Canada or majors taught at UBC. We know the categories in advance and this is one of the cases where it might be OK to violate the golden rule and get a list of all possible values for the **categorical variable**. \n",
    "\n",
    "### Raw text problems\n",
    "- Some popular representations of raw text include: \n",
    "    - **Bag of words** \n",
    "    - TF-IDF\n",
    "    - Embedding representations \n",
    "- #### Bag of words (BOW) representation\n",
    "    - One of the most popular representation of raw text \n",
    "    - Ignores the syntax and word order\n",
    "    - It has two components: \n",
    "        - The vocabulary (all unique words in all documents) \n",
    "        - A value indicating either the presence or absence or the count of each word in the document. \n",
    "- #### Extracting BOW features using `scikit-learn`\n",
    "    - `CountVectorizer`\n",
    "        - Converts a collection of text documents to a matrix of word counts.  \n",
    "        - Each row represents a \"document\" (e.g., a text message in our example). \n",
    "        - Each column represents a word in the vocabulary (the set of unique words) in the training data. \n",
    "        - Each cell represents how often the word occurs in the document.\n",
    "        - The vocabulary (mapping from feature indices to actual words) can be obtained using `get_feature_names()`\n",
    "#### Important hyperparameters of `CountVectorizer` \n",
    "\n",
    "- `binary`\n",
    "    - whether to use absence/presence feature values or counts\n",
    "- `max_features`\n",
    "    - only consider top `max_features` ordered by frequency in the corpus\n",
    "- `max_df`\n",
    "    - ignore features which occur in more than `max_df` documents \n",
    "- `min_df` \n",
    "    - ignore features which occur in less than `min_df` documents \n",
    "- `ngram_range`\n",
    "    - consider word sequences in the given range \n",
    "### Why sparse matrices? \n",
    "\n",
    "- Most words do not appear in a given document.\n",
    "- We get massive computational savings if we only store the nonzero elements.\n",
    "- There is a bit of overhead, because we also need to store the locations:\n",
    "    - e.g. \"location (3,27): 1\".\n",
    "    \n",
    "- However, if the fraction of nonzero is small, this is a huge win."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a054f9-d9e3-4885-b697-e8d3821b0cfd",
   "metadata": {},
   "source": [
    "- What's the problem with calling `fit_transform` on the test data in the context of `CountVectorizer`?\n",
    "    - will be breaking the golden rule\n",
    "    - your training data and test data are different\n",
    "- Do we need to scale after applying bag-of-words representation? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dd9250-6017-4b11-8e41-37f011956ed0",
   "metadata": {},
   "source": [
    "## Linear models\n",
    "**Linear models** is a fundamental and widely used class of models. They are called **linear** because they make a prediction using a **linear function** of the input features.  \n",
    "\n",
    "### Linear regression \n",
    "\n",
    "- Imagine a hypothetical regression problem of predicting weight of a snake given its length. \n",
    "- #### `Ridge()`\n",
    "    - complexity hyperparameter `alpha`, controls the fundamental tradeoff\n",
    "        - larger `alpha` $\\rightarrow$ likely to underfit\n",
    "        - smaller `alpha` $\\rightarrow$ likely to overfit\n",
    "- #### Prediction of linear regression\n",
    "    - The prediction will be the corresponding weight on the linear line. \n",
    "    - we can access the slope (i.e., coefficient or weight) and the intercept using `coef_` and `intercept_`, respectively. \n",
    "        - .coef_\n",
    "            - Sign\n",
    "                - Positive number: as number gets bigger, positive impact on prediction\n",
    "                - Negative number: as number gets bigger, negative impact on prediction\n",
    "            - Magnitude\n",
    "                - Bigger magnitude $\\rightarrow$ bigger impact on the prediction \n",
    "        - .intercept_\n",
    "    - Given a feature value $x_1$ and learned coefficient $w_1$ and intercept $b$, we can get the prediction $\\hat{y}$ with the following formula:\n",
    "$$\\hat{y} = w_1x_1 + b$$\n",
    "    - same value as calling .predict()\n",
    "    - ### Generalizing to more features\n",
    "        For more features, the model is a higher dimensional hyperplane and the general prediction formula looks as follows: \n",
    "\n",
    "        $\\hat{y} =$ <font color=\"red\">$w_1$</font> <font color=\"blue\">$x_1$ </font> $+ \\dots +$ <font color=\"red\">$w_d$</font> <font color=\"blue\">$x_d$</font> + <font  color=\"green\"> $b$</font>\n",
    "### Logistic regression\n",
    "\n",
    "- A linear model for **classification**. \n",
    "- Similar to linear regression, it learns weights associated with each feature and the bias. \n",
    "- It applies a **threshold** on the raw output to decide whether the class is positive or negative. \n",
    "- A linear classifier learns **weights** or **coefficients** associated with the features.  \n",
    "- So the prediction is based on the **weighted sum** of the input features.\n",
    "So a linear classifier is a linear function of the input `X`, followed by a threshold. \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "z =& w_1x_1 + \\dots + w_dx_d + b\\\\\n",
    "=& w^Tx + b\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "$$\\hat{y} = \\begin{cases}\n",
    "         1, & \\text{if } z \\geq r\\\\\n",
    "         -1, & \\text{if } z < r\n",
    "\\end{cases}$$\n",
    "- `coef_` and `intercept_`\n",
    "- The `classes_` attribute tells us which class is considered negative and which one is considered positive\n",
    "### Main hyperparameter of logistic regression \n",
    "\n",
    "- `C` is the main hyperparameter which controls the fundamental trade-off.\n",
    "    - smaller `C` $\\rightarrow$ might lead to underfitting\n",
    "    - bigger `C` $\\rightarrow$ might lead to overfitting\n",
    "### Decision boundary of logistic regression\n",
    "\n",
    "- The decision boundary of logistic regression is a **hyperplane** dividing the feature space in half. \n",
    "- For $d=2$, the decision boundary is a line (1-dimensional)\n",
    "- For $d=3$, the decision boundary is a plane (2-dimensional)\n",
    "- For $d\\gt 3$, the decision boundary is a $d-1$-dimensional hyperplane\n",
    "### Linear SVM \n",
    "\n",
    "\n",
    "- There is also a linear SVM. You can pass `kernel=\"linear\"` to create a linear SVM. \n",
    "- `predict` method of linear SVM and logistic regression works the same way. \n",
    "- We can get `coef_` associated with the features and `intercept_` using a Linear SVM model. \n",
    "### Interpretation of coefficients in linear models \n",
    "- the $j$th coefficient tells us how feature $j$ affects the prediction\n",
    "- if $w_j > 0$ then increasing $x_{ij}$ moves us toward predicting $+1$\n",
    "- if $w_j < 0$ then increasing $x_{ij}$ moves us toward prediction $-1$\n",
    "- if $w_j == 0$ then the feature is not used in making a prediction\n",
    "### Pros and Cons of Linear Models\n",
    "- #### Strengths of linear models \n",
    "    - Fast to train and predict\n",
    "    - Scale to large datasets and work well with sparse data \n",
    "    - Relatively easy to understand and interpret the predictions\n",
    "    - Perform well when there is a large number of features \n",
    "- #### Limitations of linear models \n",
    "    - Is your data \"linearly separable\"? Can you draw a hyperplane between these datapoints that separates them with 0 error. \n",
    "        - If the training examples can be separated by a linear decision rule, they are **linearly separable**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa2417b-4114-427b-bae8-0993c7721141",
   "metadata": {},
   "source": [
    "### `predict_proba`\n",
    "\n",
    "- For most of the `scikit-learn` classification models we can access this confidence score or probability score using a method called `predict_proba`.  \n",
    "- The output of `predict_proba` is the probability of each class. \n",
    "#### The sigmoid function \n",
    "- The sigmoid function \"squashes\" the raw model output from any number to the range $[0,1]$ using the following formula, where $x$ is the raw model output. \n",
    "$$\\frac{1}{1+e^{-x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f12b0b8-58a9-48d6-a79d-e534cfafdff5",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization \n",
    "- ### Automated optimizations methods\n",
    "    - #### Exhaustive grid search: [`sklearn.model_selection.GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "        - For `GridSearchCV` we need\n",
    "            - an instantiated model or a pipeline\n",
    "            - a parameter grid: A user specifies a set of values for each hyperparameter. \n",
    "            - other optional arguments \n",
    "        - can call `fit`, `predict` or `score` on it\n",
    "        - Fitting the `GridSearchCV` object \n",
    "            - Searches for the best hyperparameter values\n",
    "            - You can access the best score and the best hyperparameters using `best_score_` and `best_params_` attributes, respectively. \n",
    "        - \n",
    "        ```\n",
    "        grid_search = GridSearchCV(\n",
    "            pipe, param_grid, cv=5, n_jobs=-1, return_train_score=True\n",
    "        )\n",
    "        ```\n",
    "        - pd.DataFrame(grid_search.cv_results_)\n",
    "        - pd.DataFrame(grid_search.cv_results_).set_index(\"rank_test_score\").sort_index()\n",
    "        - `n_jobs=-1`\n",
    "        - model__hyperparameterName\n",
    "            - \"svc__gamma\": [0.001, 0.01, 0.1, 1.0, 10, 100],\n",
    "            - \"svc__C\": [0.001, 0.01, 0.1, 1.0, 10, 100],\n",
    "        - #### Problems with exhaustive grid search \n",
    "            - Required number of models to evaluate grows exponentially with the dimensionally of the configuration space. \n",
    "            - It might take a really long time\n",
    "\n",
    "    - #### Randomized search: [`sklearn.model_selection.RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n",
    "        - Samples configurations at random until certain budget (e.g., time) is exhausted \n",
    "        - \n",
    "        ```\n",
    "        random_search = RandomizedSearchCV(\n",
    "            pipe_svm, param_distributions=param_grid, n_jobs=-1, n_iter=20, cv=5, random_state=42\n",
    "        )\n",
    "        ```\n",
    "        - \n",
    "        ```\n",
    "        pd.DataFrame(random_search.cv_results_)[\n",
    "            [\n",
    "                \"mean_test_score\",\n",
    "                \"param_svc__gamma\",\n",
    "                \"param_svc__C\",\n",
    "                \"mean_fit_time\",\n",
    "                \"rank_test_score\",\n",
    "            ]\n",
    "        ].set_index(\"rank_test_score\").sort_index().T\n",
    "        ```\n",
    "        - `n_iter`\n",
    "            - Larger `n_iter` will take longer but it'll do more searching.\n",
    "        - #### Advantages of `RandomizedSearchCV`\n",
    "            - Faster compared to `GridSearchCV`.\n",
    "            - Adding parameters that do not influence the performance does not affect efficiency.\n",
    "            - Works better when some parameters are more important than others. \n",
    "    - #### Pros and Cons of Automated Optimization\n",
    "        - Advantages \n",
    "            - reduce human effort\n",
    "            - less prone to error and improve reproducibility\n",
    "            - data-driven approaches may be effective\n",
    "        - Disadvantages\n",
    "            - may be hard to incorporate intuition\n",
    "            - be careful about overfitting on the validation set <br><br>\n",
    "\n",
    "- What makes hyperparameter optimization a hard problem?\n",
    "    - because there are many models and transformers, we want to find the best\n",
    "    -  hard search problem, there are many possibilities\n",
    "- What are two different tools provide by sklearn for hyperparameter optimization?  \n",
    "    - greed search cv\n",
    "    - randomized search cv\n",
    "        - not commited to run all experiements exhaustively\n",
    "        - provide some distribution instead of greed\n",
    "- What is optimization bias? \n",
    "    - overfit on validation set, found a hyperparameter that is not "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066bd8da-ddc6-42e5-ac93-dd146732d6fa",
   "metadata": {},
   "source": [
    "## Evaluation metrics\n",
    "- ### `.score` by default returns accuracy which is \n",
    "    \n",
    "    - $$\\frac{\\text{correct predictions}}{\\text{total examples}}$$\n",
    "    \n",
    "    - misleading when you have class imbalance\n",
    "    \n",
    "- ### Confusion matrix \n",
    "    - confusion_matrix(y_train, cross_val_predict(pipe, X_train, y_train))\n",
    "    - \n",
    "    ```\n",
    "        TN, FP, FN, TP = confusion_matrix(y_valid, predictions).ravel()\n",
    "        plot_confusion_matrix_example(TN, FP, FN, TP)\n",
    "    ```\n",
    "- ### Recall\n",
    "    - Among all positive examples, how many did you identify?\n",
    "    $$ recall = \\frac{TP}{TP+FN} = \\frac{TP}{\\#positives} $$\n",
    "- ### Precision \n",
    "    - Among the positive examples you identified, how many were actually positive?\n",
    "    $$ precision = \\frac{TP}{TP+FP}$$\n",
    "- ### F1-score\n",
    "    - F1-score combines precision and recall to give one score, which could be used in hyperparameter optimization, for instance. \n",
    "    - F1-score is a harmonic mean of precision and recall. \n",
    "    $$ f1 = 2 \\times \\frac{ precision \\times recall}{precision + recall}$$\n",
    "- ### Macro average\n",
    "    - You give equal importance to all classes and average over all classes.  \n",
    "- ### Weighted average\n",
    "    - Weighted by the number of samples in each class. \n",
    "    - Divide by the total number of samples. \n",
    "- ### Cross validation with different metrics\n",
    "    ```\n",
    "    scoring = [\n",
    "        \"accuracy\",\n",
    "        \"f1\",\n",
    "        \"recall\",\n",
    "        \"precision\",\n",
    "    ]  # scoring can be a string, a list, or a dictionary\n",
    "    pipe = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "    scores = cross_validate(\n",
    "        pipe, X_train_big, y_train_big, return_train_score=True, scoring=scoring\n",
    "    )\n",
    "    pd.DataFrame(scores)\n",
    "    ```\n",
    "- ### Sklearn API\n",
    "    - accuracy: accuracy_score(y_valid, pipe_lr.predict(X_valid)))\n",
    "    - error: 1 - accuracy_score(y_valid, pipe_lr.predict(X_valid)))\n",
    "    - precision: precision_score(y_valid, pipe_lr.predict(X_valid), zero_division=1)\n",
    "    - recall: recall_score(y_valid, pipe_lr.predict(X_valid))\n",
    "    - f1 score: f1_score(y_valid, pipe_lr.predict(X_valid))\n",
    "    - classification report that gives all the above info\n",
    "    ```\n",
    "    print(\n",
    "        classification_report(\n",
    "            y_valid, pipe_lr.predict(X_valid), target_names=[\"non-fraud\", \"fraud\"]\n",
    "        )\n",
    "    )\n",
    "    ```\n",
    "- ### Precision/Recall tradeoff \n",
    "    - But there is a trade-off between precision and recall. \n",
    "    - If you identify more things as \"fraud\", recall is going to increase but there are likely to be more false positives. \n",
    "    - #### Decreasing the threshold\n",
    "        - Decreasing the threshold means a lower bar for predicting fraud. \n",
    "            - You are willing to risk more false positives in exchange of more true positives. \n",
    "            - recall would either stay the same or go up and precision is likely to go down\n",
    "            - occasionally, precision may increase if all the new examples after decreasing the threshold are TPs. \n",
    "    - #### Increasing the threshold\n",
    "        - Increasing the threshold means a higher bar for predicting fraud. \n",
    "            - recall would go down or stay the same but precision is likely to go up \n",
    "            - occasionally, precision may go down as the denominator for precision is TP+FP.     \n",
    "    - #### Precision-recall curve\n",
    "        - The top-right would be a perfect classifier (precision = recall = 1).\n",
    "        - Usually the goal is to keep recall high as precision goes up. \n",
    "    ```\n",
    "    precision, recall, thresholds = precision_recall_curve(\n",
    "        y_valid, pipe_lr.predict_proba(X_valid)[:, 1]\n",
    "    )\n",
    "    plt.plot(precision, recall, label=\"logistic regression: PR curve\")\n",
    "    plt.xlabel(\"Precision\")\n",
    "    plt.ylabel(\"Recall\")\n",
    "    plt.plot(\n",
    "        precision_score(y_valid, pipe_lr.predict(X_valid)),\n",
    "        recall_score(y_valid, pipe_lr.predict(X_valid)),\n",
    "        \"or\",\n",
    "        markersize=10,\n",
    "        label=\"threshold 0.5\",\n",
    "    )\n",
    "    plt.legend(loc=\"best\");\n",
    "    ```\n",
    "    - #### AP score \n",
    "        - one number summarizing the PR plot (e.g., in hyperparameter optimization)\n",
    "        - This is called **average precision** (AP score)\n",
    "        - AP score has a value between 0 (worst) and 1 (best). \n",
    "        -\n",
    "        ```\n",
    "        ap_lr = average_precision_score(y_valid, pipe_lr.predict_proba(X_valid)[:, 1])\n",
    "        print(\"Average precision of logistic regression: {:.3f}\".format(ap_lr))\n",
    "        ```\n",
    "        - \n",
    "        ```\n",
    "        PrecisionRecallDisplay.from_estimator(pipe_lr, X_valid, y_valid);\n",
    "        ```\n",
    "- ### AP vs. F1-score\n",
    "    - F1 score is for a given threshold and measures the quality of `predict`.\n",
    "    - AP score is a summary across thresholds and measures the quality of `predict_proba`.\n",
    "- ### Receiver Operating Characteristic (ROC) curve \n",
    "\n",
    "- Another commonly used tool to analyze the behavior of classifiers at different thresholds.  \n",
    "- Similar to PR curve, it considers all possible thresholds for a given classifier given by `predict_proba` but instead of precision and recall it plots false positive rate (FPR) and true positive rate (TPR or recall).\n",
    "$$ TPR = \\frac{TP}{TP + FN}, FPR  = \\frac{FP}{FP + TN}$$\n",
    "- The ideal curve is close to the top left\n",
    "    - Ideally, you want a classifier with high recall while keeping low false positive rate.  \n",
    "```\n",
    "fpr, tpr, thresholds = roc_curve(y_valid, pipe_lr.predict_proba(X_valid)[:, 1])\n",
    "plt.plot(fpr, tpr, label=\"ROC Curve\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR (recall)\")\n",
    "\n",
    "default_threshold = np.argmin(np.abs(thresholds - 0.5))\n",
    "\n",
    "plt.plot(\n",
    "    fpr[default_threshold],\n",
    "    tpr[default_threshold],\n",
    "    \"or\",\n",
    "    markersize=10,\n",
    "    label=\"threshold 0.5\",\n",
    ")\n",
    "plt.legend(loc=\"best\");\n",
    "```\n",
    "- ### Area under the curve (AUC)\n",
    "    - AUC provides a single meaningful number for the model performance. \n",
    "    ```\n",
    "    roc_lr = roc_auc_score(y_valid, pipe_lr.predict_proba(X_valid)[:, 1])\n",
    "    roc_svc = roc_auc_score(y_valid, pipe_svc.decision_function(X_valid))\n",
    "    print(\"AUC for LR: {:.3f}\".format(roc_lr))\n",
    "    print(\"AUC for SVC: {:.3f}\".format(roc_svc))\n",
    "    ```\n",
    "    - AUC of 0.5 means random chance. \n",
    "    - AUC can be interpreted as evaluating the **ranking** of positive examples.\n",
    "    - What's the probability that a randomly picked positive point has a higher score according to the classifier than a randomly picked point from the negative class. \n",
    "    - AUC of 1.0 means all positive points have a higher score than all negative points. \n",
    "    ```\n",
    "    RocCurveDisplay.from_estimator(pipe_lr, X_valid, y_valid);\n",
    "    ```\n",
    "    \n",
    "\n",
    "<br><br> \n",
    "- Why accuracy is not always enough?\n",
    "    - if class imbalance, accuracy is misleading\n",
    "    - care about how well you are performing in the class that you are interested in \n",
    "- Why it's useful to get prediction probabilities? \n",
    "- In what scenarios do you care more about precision or recall? \n",
    "    - recall: when you care about false negative, minimize it\n",
    "    - precision: false positive\n",
    "    - use f1 score to balance precision and recall\n",
    "- What's the main difference between AP score and F1 score?\n",
    "    - changing threshold \n",
    "    - F1 is when threshold is 0.5\n",
    "    - AP is calculated over a number of thresholds, quality of predict proba\n",
    "- What are advantages of RMSE or MAPE over MSE? \n",
    "    - interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7c7743-e14f-4144-a1d4-21b1c293db5e",
   "metadata": {},
   "source": [
    "### Class imbalance in training sets\n",
    "\n",
    "- This typically refers to having many more examples of one class than another in one's training set.\n",
    "- Real world data is often imbalanced. \n",
    "    - Our Credit Card Fraud dataset is imbalanced.\n",
    "    - Ad clicking data is usually drastically imbalanced. (Only around ~0.01% ads are clicked.)\n",
    "    - Spam classification datasets are also usually imbalanced.\n",
    "### Handling imbalance\n",
    "There are two common approaches for this: \n",
    "- **Changing the data (optional)** (not covered in this course)\n",
    "   - Undersampling\n",
    "   - Oversampling \n",
    "       - Random oversampling\n",
    "       - SMOTE \n",
    "- **Changing the training procedure** \n",
    "    - `class_weight`\n",
    "        - All `sklearn` classifiers have a parameter called `class_weight`.\n",
    "        - This allows you to specify that one class is more important than another.\n",
    "    - `class_weight=\"balanced\"`\n",
    "        - A useful setting is `class_weight=\"balanced\"`.\n",
    "        - This sets the weights so that the classes are \"equal\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaa9076-0510-4fe6-9d3d-9be87232c6fb",
   "metadata": {},
   "source": [
    "### `RidgeCV`\n",
    "\n",
    "- automatically tunes `alpha` based on cross-validation.\n",
    "```\n",
    "ridgecv_pipe = make_pipeline(preprocessor, RidgeCV(alphas=alphas, cv=10))\n",
    "ridgecv_pipe.fit(X_train, y_train);\n",
    "best_alpha = ridgecv_pipe.named_steps['ridgecv'].alpha_\n",
    "best_alpha\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab5d0cf-8b0f-447e-bb9b-47c38131200f",
   "metadata": {},
   "source": [
    "## Scoring functions for regression\n",
    "\n",
    "- ### `mean squared error (MSE)`\n",
    "    - MSE (mean squared error) is in units of target squared, hard to interpret; 0 is best\n",
    "    ```\n",
    "    mean_squared_error(y_train, lr_tuned.predict(X_train))\n",
    "    ```\n",
    "- ### `$R^2$`\n",
    "    - $R^2$ is the default .score(), it is unitless, 0 is bad, 1 is best\n",
    "    - The maximum is 1 for perfect predictions\n",
    "    - Negative values are very bad: \"worse than DummyRegressor\" (very bad)\n",
    "- ### `root mean squared error (RMSE)`\n",
    "    - RMSE (root mean squared error) is in the same units as the target; 0 is best\n",
    "    ```\n",
    "    np.sqrt(mean_squared_error(y_train, lr_tuned.predict(X_train)))\n",
    "    ```\n",
    "- ### `Mean absolute percent error (MAPE)`\n",
    "    - MAPE (mean absolute percent error) is unitless; 0 is best, 1 is bad\n",
    "    ```\n",
    "    mean_absolute_percentage_error(y_train, pred_train)\n",
    "    ```\n",
    "    - to reduce MAPE\n",
    "    ```\n",
    "    ttr = TransformedTargetRegressor(\n",
    "    Ridge(alpha=best_alpha), func=np.log1p, inverse_func=np.expm1\n",
    "    ) # transformer for log transforming the target\n",
    "    ttr_pipe = make_pipeline(preprocessor, ttr)\n",
    "    ```\n",
    "- ### Sklearn API\n",
    "    ```\n",
    "    pd.DataFrame(\n",
    "        cross_validate(\n",
    "            lr_tuned,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            return_train_score=True,\n",
    "            scoring=[\"neg_mean_squared_error\", \"neg_mean_absolute_percentage_error\"]\n",
    "        )\n",
    "    )\n",
    "    ```\n",
    "    ```\n",
    "    \n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e6c11b-ee26-4d37-87ea-2b6bd3560f60",
   "metadata": {},
   "source": [
    "## Ensembles\n",
    "- **Ensembles** are models that combine multiple machine learning models to create more powerful models. \n",
    "- ### `RandomForestClassifier` \n",
    "    - A single decision tree is likely to overfit\n",
    "    - Use a collection of diverse decision trees\n",
    "    - Each tree overfits on some part of the data but we can reduce overfitting by averaging the results \n",
    "    - `n_estimators`: number of decision trees (higher = more complexity)\n",
    "    - `max_depth`: max depth of each decision tree (higher = more complexity)\n",
    "    - `max_features`: the number of features you get to look at each split (higher = more complexity)\n",
    "- ### Strengths and weaknesses\n",
    "    - Strengths\n",
    "        - Usually one of the best performing off-the-shelf classifiers without heavy tuning of hyperparameters\n",
    "        - Don't require scaling of data \n",
    "        - Less likely to overfit \n",
    "        - Slower than decision trees because we are fitting multiple trees but can easily parallelize training because all trees are independent of each other (that said, sklearn implementation is kind of slow)\n",
    "        - In general, able to capture a much broader picture of the data compared to a single decision tree. \n",
    "    - Weaknesses\n",
    "        - Require more memory \n",
    "        - Hard to interpret\n",
    "        - Tend not to perform well on high dimensional sparse data such as text data\n",
    "    \n",
    "<br><br>\n",
    "- How does a random forest model inject randomness in the model?\n",
    "- What's the difference between random forests and gradient boosted trees?\n",
    "- Why do we need averaging or stacking? \n",
    "- What are the benefits of stacking over averaging?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fe7d3b-ec86-4ec3-86b8-f7ef6d0b1b79",
   "metadata": {},
   "source": [
    "### Feature importances and selection \n",
    "\n",
    "- What are the limitations of looking at simple correlations between features and targets? \n",
    "- How can you get feature importances or non-linear models?\n",
    "- What you might need to explain a single prediction?\n",
    "- What's the difference between feature engineering and feature selection? \n",
    "- Why do we need feature selection?\n",
    "- What are the three possible ways we looked at for feature selection? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e97d341-aea3-4045-8d88-1ef17b1e8484",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330]",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
